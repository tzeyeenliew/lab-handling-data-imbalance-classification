{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ba8d1a",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "#### You are working as an analyst with this internet service provider. You are provided with this historical data about your company's customers and their churn trends. Your task is to build a machine learning model that will help the company identify customers that are more likely to default/churn and thus prevent losses from such customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b058a25",
   "metadata": {},
   "source": [
    "### <span style=\"color:purple\">For this lab, I would like to focus more on the impact of various scaling methods on oversampling/undersampling process, instead of focusing on the oversampling/undersampling methods and F1/recall values etc. (as I have already done that in the ealier data imbalance lab!)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b56d1",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "#### In this lab, we will first take a look at the degree of imbalance in the data and correct it using the techniques we learned on the class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f938f7",
   "metadata": {},
   "source": [
    "#### 1. Import the required libraries and modules that you would need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "044addc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acf8f2",
   "metadata": {},
   "source": [
    "#### 2. Read that data into Python and call the dataframe churnData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c4e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "churnData= pd.read_csv('Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0c56f",
   "metadata": {},
   "source": [
    "#### 3. Check the datatypes of all the columns in the data. You would see that the column TotalCharges is object type. Convert this column into numeric type using pd.to_numeric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f16969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges         object\n",
      "Churn                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(churnData.dtypes) #TotalCharges is listed as an object, which is inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8410315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "churnData['TotalCharges']= pd.to_numeric(churnData['TotalCharges'], errors = 'coerce' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2887bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges        float64\n",
      "Churn                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(churnData.dtypes) #To check if the data type conversion is successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa687ae",
   "metadata": {},
   "source": [
    "#### 4.Check for null values in the dataframe. Replace the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6525ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender               0\n",
      "SeniorCitizen        0\n",
      "Partner              0\n",
      "Dependents           0\n",
      "tenure               0\n",
      "PhoneService         0\n",
      "OnlineSecurity       0\n",
      "OnlineBackup         0\n",
      "DeviceProtection     0\n",
      "TechSupport          0\n",
      "StreamingTV          0\n",
      "StreamingMovies      0\n",
      "Contract             0\n",
      "MonthlyCharges       0\n",
      "TotalCharges        11\n",
      "Churn                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(churnData.isnull().sum()) #TotalCharges has 11 nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337dfe1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean     2283.300441\n",
       "std      2266.771362\n",
       "min        18.800000\n",
       "25%       401.450000\n",
       "50%      1397.475000\n",
       "75%      3794.737500\n",
       "max      8684.800000\n",
       "Name: TotalCharges, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churnData['TotalCharges'].describe() #dropping 11 nulls would not affect the the column that much, as it has 7032 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762085dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "churnData.dropna(subset=['TotalCharges'], inplace=True) #dropping nulls from TotalCharges\n",
    "print(churnData.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eee76",
   "metadata": {},
   "source": [
    "#### 4.Use the following features: tenure, SeniorCitizen, MonthlyCharges and TotalCharges:\n",
    "#### Scale the features either by using normalizer or a standard scaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8ad0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['tenure', 'SeniorCitizen', 'MonthlyCharges', 'TotalCharges'] #scales the values of the variables so that they fall between 0 and 1\n",
    "X= churnData[features]\n",
    "y= churnData['Churn'] #As the question is regarding customer churn\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "X_scaled= scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cd449",
   "metadata": {},
   "source": [
    "#### Split the data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0af999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d37e26",
   "metadata": {},
   "source": [
    "#### Fit a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82fa2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticregression= LogisticRegression()\n",
    "logisticregression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc43ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= logisticregression.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878425a",
   "metadata": {},
   "source": [
    "#### Check the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8078ac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc02c09",
   "metadata": {},
   "source": [
    "### Managing imbalance in the dataset\n",
    "\n",
    "#### Check for the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114af742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No     5163\n",
      "Yes    1869\n",
      "Name: Churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(churnData['Churn'].value_counts()) # from the results below,there seems to be a huge imbalance in the representation of the two categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2b43e",
   "metadata": {},
   "source": [
    "#### Use the resampling strategies used in class for upsampling and downsampling to create a balance between the two classes. Each time fit the model and see how the accuracy of the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0540108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_test, y_test): #this function is meant to speed up the fitting process after train_test_split, if needed \n",
    "    logisticregression = LogisticRegression()\n",
    "    logisticregression.fit(X_train, y_train) #fits a logreg model on the training data\n",
    "    prediction = logisticregression.predict(X_test) #prediction = y_pred\n",
    "    accuracy = accuracy_score(y_test, prediction) #calculates the accuracy score\n",
    "    return logisticregression, accuracy #returns the trained model and the corresponding accuracy score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608695d0",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "#### SMOTE creates as many fake samples from the minority class as needed in order to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6607d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8fc6f",
   "metadata": {},
   "source": [
    "##### I've decided to look into each predicting feature in detail to see if a separate scaling method should be applied instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8183eb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean       32.421786\n",
       "std        24.545260\n",
       "min         1.000000\n",
       "25%         9.000000\n",
       "50%        29.000000\n",
       "75%        55.000000\n",
       "max        72.000000\n",
       "Name: tenure, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churnData['tenure'].describe()  #Tenure has a large range of values, thus StandardScaler would be more apt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19c46f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean        0.162400\n",
       "std         0.368844\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: SeniorCitizen, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churnData['SeniorCitizen'].describe() # Senior Citizen is a binary variable, thus no scaling is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c811d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean       64.798208\n",
       "std        30.085974\n",
       "min        18.250000\n",
       "25%        35.587500\n",
       "50%        70.350000\n",
       "75%        89.862500\n",
       "max       118.750000\n",
       "Name: MonthlyCharges, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churnData['MonthlyCharges'].describe() # Monthly Charges has a specific range of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1858220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean     2283.300441\n",
       "std      2266.771362\n",
       "min        18.800000\n",
       "25%       401.450000\n",
       "50%      1397.475000\n",
       "75%      3794.737500\n",
       "max      8684.800000\n",
       "Name: TotalCharges, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churnData['TotalCharges'].describe() # Total Charges has a large range of values, thus using Standard Scaler to standardize it would be more apt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583041f",
   "metadata": {},
   "source": [
    "#### Scaling the Variables and applying SMOTE/TOMEKlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc3c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= churnData[['tenure', 'SeniorCitizen', 'MonthlyCharges', 'TotalCharges']] #defines the X feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51efd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= churnData['Churn'] # separating the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d00b48da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #applying the standardscaler to the intended variables\n",
    "X[['tenure', 'TotalCharges']]= scaler.fit_transform(X[['tenure', 'TotalCharges']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85eb2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler= MinMaxScaler() #applying the minmaxscaler to the intended variables\n",
    "X['MonthlyCharges']= minmax_scaler.fit_transform(X['MonthlyCharges'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684a5f2",
   "metadata": {},
   "source": [
    "#### SMOTE with combined scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39837f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote= SMOTE()  #oversampling the minoroty class with SMOTE\n",
    "X_sm, y_sm= smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e2a063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     5163\n",
       "Yes    5163\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sm.value_counts() #to check if the data is balanced by the SMOTE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "facb08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09ae0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticregression_model, accuracy = train_logistic_regression(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c21f39ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with combined scaling and SMOTE): 0.7333763718528082\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (with combined scaling and SMOTE):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a763e50",
   "metadata": {},
   "source": [
    "### TOMEKlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbf720",
   "metadata": {},
   "source": [
    "#### Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6acc2e",
   "metadata": {},
   "source": [
    "#### TOMEKlinks with combined scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a4a00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d77b7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b6ac665",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_links = TomekLinks() #undersampling the majority class with TOMEKlinks\n",
    "X_tomek, y_tomek = tomek_links.fit_resample(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a66867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tomek, X_test_tomek, y_train_tomek, y_test_tomek= train_test_split(X_tomek, y_tomek, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33921a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticregression_tomek= LogisticRegression() #decided not to use the earlier predefined function as its easier to remember that this involves TOM\n",
    "logisticregression_tomek.fit(X_train_tomek, y_train_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bfe9d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with combined scaling and TOMEKlinks) 0.7730769230769231\n"
     ]
    }
   ],
   "source": [
    "y_pred_tomek= logisticregression_tomek.predict(X_test_tomek)\n",
    "accuracy_tomek= accuracy_score(y_test_tomek, y_pred_tomek)\n",
    "print(\"Accuracy (with combined scaling and TOMEKlinks)\", accuracy_tomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3a9b1",
   "metadata": {},
   "source": [
    "#### Unfortunately, my combined scaling strategy actually resulted in a lower accuracy score than before -- in both instances of SMOTE and TOMEKlinks. Hence I will try to reiterate the process by applying only one method on all X variables at a time and then combining the accuracy scores of both approaches. However, it is interesting to note that the TOMEK model outperformed the SMOTE model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5aebe",
   "metadata": {},
   "source": [
    "#### Undoing the Scaling Process using inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "443e8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['tenure', 'TotalCharges']]=scaler.inverse_transform(X[['tenure', 'TotalCharges']]) #inverse transforming variables transformed using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d1cfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['MonthlyCharges']= minmax_scaler.inverse_transform(X['MonthlyCharges'].values.reshape(-1, 1)) #inverse transforming variable that was transformed using MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800adb1",
   "metadata": {},
   "source": [
    "#### Scaling the Variables using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a4d240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= churnData[['tenure', 'TotalCharges', 'SeniorCitizen', 'MonthlyCharges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e415950",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = churnData['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af306ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['tenure', 'TotalCharges', 'SeniorCitizen', 'MonthlyCharges']]= scaler.fit_transform(X[['tenure', 'TotalCharges', 'SeniorCitizen', 'MonthlyCharges']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da103af6",
   "metadata": {},
   "source": [
    "#### SMOTE with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13d9b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote= SMOTE() \n",
    "X_sm, y_sm= smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "060f03df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     5163\n",
       "Yes    5163\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sm.value_counts() #to check if the data is balanced by the SMOTE proces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6075a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "515d0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticregression_model, accuracy = train_logistic_regression(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16f4dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with StandardScaler and SMOTE): 0.7320852162685604\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (with StandardScaler and SMOTE):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_smote2 = classification_report(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de380d5d",
   "metadata": {},
   "source": [
    "#### TOMEKlinks with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5178ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d156ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_links = TomekLinks() #undersampling the majority class with TOMEKlinks\n",
    "X_tomek, y_tomek = tomek_links.fit_resample(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64146fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tomek, X_test_tomek, y_train_tomek, y_test_tomek= train_test_split(X_tomek, y_tomek, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10fbbfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticregression_tomek= LogisticRegression() #decided not to use the earlier predefined function as its easier to remember that this involves TOM\n",
    "logisticregression_tomek.fit(X_train_tomek, y_train_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e3b1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with combined scaling and TOMEKlinks) 0.7868098159509203\n"
     ]
    }
   ],
   "source": [
    "y_pred_tomek= logisticregression_tomek.predict(X_test_tomek)\n",
    "accuracy_tomek= accuracy_score(y_test_tomek, y_pred_tomek)\n",
    "print(\"Accuracy (with StandardScaler and TOMEKlinks)\", accuracy_tomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16380a",
   "metadata": {},
   "source": [
    "#### SMOTE with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89b97f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['tenure', 'TotalCharges','SeniorCitizen', 'MonthlyCharges']]=scaler.inverse_transform(X[['tenure', 'TotalCharges', 'SeniorCitizen', 'MonthlyCharges']]) #inverse transforming variables transformed using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e75c5375",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['tenure'] = minmax_scaler.fit_transform(X['tenure'].values.reshape(-1, 1))\n",
    "X['TotalCharges'] = minmax_scaler.fit_transform(X['TotalCharges'].values.reshape(-1, 1))\n",
    "X['SeniorCitizen'] = minmax_scaler.fit_transform(X['SeniorCitizen'].values.reshape(-1, 1))\n",
    "X['MonthlyCharges'] = minmax_scaler.fit_transform(X['MonthlyCharges'].values.reshape(-1, 1)) #mass transforming them didn't work, so I transformed them one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ce84a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote= SMOTE()  #oversampling the minoroty class with SMOTE\n",
    "X_sm, y_sm= smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed557214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     5163\n",
       "Yes    5163\n",
       "Name: Churn, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sm.value_counts() #to check if the data is balanced by the SMOTE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28a941fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1f73a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticregression_model, accuracy = train_logistic_regression(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e609e9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with MinMaxScaler and SMOTE): 0.7304712717882504\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy (with MinMaxScaler and SMOTE):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf05a4e",
   "metadata": {},
   "source": [
    "#### TomekLINKS with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "489f9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36c17174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_links = TomekLinks() #undersampling the majority class with TOMEKlinks\n",
    "X_tomek, y_tomek = tomek_links.fit_resample(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a254d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tomek, X_test_tomek, y_train_tomek, y_test_tomek= train_test_split(X_tomek, y_tomek, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11cf653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticregression_tomek= LogisticRegression() #decided not to use the earlier predefined function as its easier to remember that this involves TOM\n",
    "logisticregression_tomek.fit(X_train_tomek, y_train_tomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fad75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with MinMaxScaler and TOMEKlinks) 0.7966231772831927\n"
     ]
    }
   ],
   "source": [
    "y_pred_tomek= logisticregression_tomek.predict(X_test_tomek)\n",
    "accuracy_tomek= accuracy_score(y_test_tomek, y_pred_tomek)\n",
    "print(\"Accuracy (with MinMaxScaler and TOMEKlinks)\", accuracy_tomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e762993",
   "metadata": {},
   "source": [
    "### Conclusion: Scaling the predictive variables using the MinMaxScaler and then undersampling the majority class with TOMEKLinks seems to produce the model with the best accuracy scores. More time would be needed to also look into the classification reports of each method/combination used so far, as well as manual oversamplnig/undersampling methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
